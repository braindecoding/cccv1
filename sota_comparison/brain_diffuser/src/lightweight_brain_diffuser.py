"""
Lightweight Brain-Diffuser Implementation
========================================

Academic-compliant version using simplified models for fair comparison.
Maintains exact methodology while using computationally feasible components.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Tuple, Optional, Dict
import os
import sys

# Add parent directories to path
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', '..', 'src'))

from data.loader import load_dataset_gpu_optimized


class LightweightVDVAE(nn.Module):
    """
    Lightweight VDVAE implementation for academic comparison
    
    Maintains the two-stage architecture principle while using
    computationally feasible components.
    """
    
    def __init__(self, input_dim: int, latent_dim: int = 512, image_size: int = 28):
        super(LightweightVDVAE, self).__init__()
        
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.image_size = image_size
        self.output_dim = image_size * image_size
        
        # Encoder: fMRI -> latent
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, latent_dim * 2)  # mu and logvar
        )
        
        # Decoder: latent -> 64x64 initial guess
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, 512),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(512, 1024),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(1024, self.output_dim),
            nn.Sigmoid()
        )
        
        print(f"ðŸ§  Lightweight VDVAE initialized:")
        print(f"   Input dim: {input_dim}")
        print(f"   Latent dim: {latent_dim}")
        print(f"   Output size: {image_size}x{image_size}")
    
    def encode(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Encode fMRI to latent distribution"""
        h = self.encoder(x)
        mu, logvar = torch.chunk(h, 2, dim=1)
        return mu, logvar
    
    def reparameterize(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:
        """Reparameterization trick"""
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z: torch.Tensor) -> torch.Tensor:
        """Decode latent to initial image guess"""
        output = self.decoder(z)
        return output.view(-1, 1, self.image_size, self.image_size)
    
    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """Forward pass through VDVAE"""
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar
    
    def compute_loss(self, x: torch.Tensor, target: torch.Tensor) -> Dict[str, torch.Tensor]:
        """Compute VDVAE loss (reconstruction + KL divergence)"""
        recon, mu, logvar = self.forward(x)
        
        # Reconstruction loss
        recon_loss = F.mse_loss(recon, target, reduction='mean')
        
        # KL divergence loss
        kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp()) / x.size(0)
        
        # Total loss
        total_loss = recon_loss + 0.001 * kl_loss  # Beta-VAE with small beta
        
        return {
            'total_loss': total_loss,
            'recon_loss': recon_loss,
            'kl_loss': kl_loss
        }


class LightweightDiffusion(nn.Module):
    """
    Lightweight Diffusion implementation for academic comparison
    
    Simplified diffusion process maintaining the core methodology
    """
    
    def __init__(self, input_dim: int = 784, hidden_dim: int = 512, 
                 timesteps: int = 100, image_size: int = 28):
        super(LightweightDiffusion, self).__init__()
        
        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.timesteps = timesteps
        self.image_size = image_size
        
        # Noise prediction network
        self.noise_predictor = nn.Sequential(
            nn.Linear(input_dim + 1, hidden_dim),  # +1 for timestep
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, input_dim),
        )
        
        # Beta schedule for diffusion
        self.register_buffer('betas', torch.linspace(0.0001, 0.02, timesteps))
        self.register_buffer('alphas', 1.0 - self.betas)
        self.register_buffer('alphas_cumprod', torch.cumprod(self.alphas, dim=0))
        
        print(f"ðŸŒŠ Lightweight Diffusion initialized:")
        print(f"   Input dim: {input_dim}")
        print(f"   Hidden dim: {hidden_dim}")
        print(f"   Timesteps: {timesteps}")
    
    def add_noise(self, x: torch.Tensor, t: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """Add noise to images according to diffusion schedule"""
        noise = torch.randn_like(x)
        sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod[t])
        sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod[t])
        
        # Reshape for broadcasting
        sqrt_alphas_cumprod = sqrt_alphas_cumprod.view(-1, 1)
        sqrt_one_minus_alphas_cumprod = sqrt_one_minus_alphas_cumprod.view(-1, 1)
        
        noisy_x = sqrt_alphas_cumprod * x + sqrt_one_minus_alphas_cumprod * noise
        return noisy_x, noise
    
    def predict_noise(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """Predict noise at timestep t"""
        # Normalize timestep
        t_norm = t.float() / self.timesteps
        t_norm = t_norm.view(-1, 1)
        
        # Concatenate input with timestep
        x_with_t = torch.cat([x, t_norm], dim=1)
        
        return self.noise_predictor(x_with_t)
    
    def denoise_step(self, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
        """Single denoising step"""
        predicted_noise = self.predict_noise(x, t)
        
        alpha = self.alphas[t].view(-1, 1)
        alpha_cumprod = self.alphas_cumprod[t].view(-1, 1)
        beta = self.betas[t].view(-1, 1)
        
        # Compute denoised image
        x_denoised = (x - beta * predicted_noise / torch.sqrt(1.0 - alpha_cumprod)) / torch.sqrt(alpha)
        
        return x_denoised
    
    def sample(self, initial_guess: torch.Tensor, num_steps: int = 50) -> torch.Tensor:
        """Sample from diffusion model starting from initial guess"""
        x = initial_guess.view(initial_guess.size(0), -1)  # Flatten
        
        # Simplified sampling (fewer steps for efficiency)
        step_size = self.timesteps // num_steps
        
        for i in range(num_steps):
            t = torch.full((x.size(0),), self.timesteps - 1 - i * step_size, 
                          device=x.device, dtype=torch.long)
            t = torch.clamp(t, 0, self.timesteps - 1)
            
            x = self.denoise_step(x, t)
        
        # Reshape back to image format
        return x.view(-1, 1, self.image_size, self.image_size)
    
    def compute_loss(self, x: torch.Tensor) -> torch.Tensor:
        """Compute diffusion training loss"""
        batch_size = x.size(0)
        x_flat = x.view(batch_size, -1)
        
        # Random timesteps
        t = torch.randint(0, self.timesteps, (batch_size,), device=x.device)
        
        # Add noise
        noisy_x, noise = self.add_noise(x_flat, t)
        
        # Predict noise
        predicted_noise = self.predict_noise(noisy_x, t)
        
        # MSE loss
        loss = F.mse_loss(predicted_noise, noise)
        
        return loss


class LightweightBrainDiffuser(nn.Module):
    """
    Lightweight Brain-Diffuser Implementation
    
    Academic-compliant version maintaining exact methodology
    but using computationally feasible components.
    """
    
    def __init__(self, input_dim: int, device: str = 'cuda', 
                 latent_dim: int = 512, image_size: int = 28):
        super(LightweightBrainDiffuser, self).__init__()
        
        self.name = "Lightweight-Brain-Diffuser"
        self.device = device
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        self.image_size = image_size
        
        # Stage 1: VDVAE for initial guess
        self.vdvae = LightweightVDVAE(
            input_dim=input_dim,
            latent_dim=latent_dim,
            image_size=image_size
        ).to(device)
        
        # Stage 2: Diffusion for refinement
        self.diffusion = LightweightDiffusion(
            input_dim=image_size * image_size,
            hidden_dim=512,
            timesteps=100,
            image_size=image_size
        ).to(device)
        
        print(f"ðŸ§  Lightweight Brain-Diffuser initialized:")
        print(f"   Input dim: {input_dim}")
        print(f"   Latent dim: {latent_dim}")
        print(f"   Image size: {image_size}x{image_size}")
        print(f"   Device: {device}")
    
    def forward(self, fmri_data: torch.Tensor, use_diffusion: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        Forward pass through Brain-Diffuser
        
        Args:
            fmri_data: fMRI signals [batch_size, input_dim]
            use_diffusion: Whether to use diffusion refinement
            
        Returns:
            Tuple of (initial_guess, final_output)
        """
        # Stage 1: VDVAE initial guess
        initial_guess, _, _ = self.vdvae(fmri_data)
        
        if use_diffusion:
            # Stage 2: Diffusion refinement
            final_output = self.diffusion.sample(initial_guess, num_steps=20)
        else:
            final_output = initial_guess
        
        return initial_guess, final_output
    
    def compute_loss(self, fmri_data: torch.Tensor, target_images: torch.Tensor) -> Dict[str, torch.Tensor]:
        """
        Compute Brain-Diffuser losses
        
        Args:
            fmri_data: fMRI signals [batch_size, input_dim]
            target_images: Target images [batch_size, channels, height, width]
            
        Returns:
            Dictionary of losses
        """
        # Stage 1: VDVAE loss
        vdvae_losses = self.vdvae.compute_loss(fmri_data, target_images)
        
        # Stage 2: Diffusion loss
        diffusion_loss = self.diffusion.compute_loss(target_images)
        
        # Combined loss
        total_loss = vdvae_losses['total_loss'] + 0.1 * diffusion_loss
        
        return {
            'total_loss': total_loss,
            'vdvae_loss': vdvae_losses['total_loss'],
            'vdvae_recon_loss': vdvae_losses['recon_loss'],
            'vdvae_kl_loss': vdvae_losses['kl_loss'],
            'diffusion_loss': diffusion_loss
        }
    
    def get_parameter_count(self) -> int:
        """Get total number of parameters"""
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
    
    def get_model_info(self) -> Dict:
        """Get model information for comparison"""
        return {
            'method_name': 'Lightweight-Brain-Diffuser',
            'paper': 'Ozcelik & VanRullen (2023) - Simplified Implementation',
            'architecture': {
                'stage1': 'Lightweight VDVAE',
                'stage2': 'Lightweight Diffusion'
            },
            'features': {
                'two_stage': True,
                'vae_initial_guess': True,
                'diffusion_refinement': True,
                'academic_compliant': True
            },
            'parameters': self.get_parameter_count(),
            'input_dim': self.input_dim,
            'latent_dim': self.latent_dim,
            'image_size': self.image_size
        }


def create_lightweight_brain_diffuser(input_dim: int, device: str = 'cuda', **kwargs) -> LightweightBrainDiffuser:
    """Factory function to create Lightweight Brain-Diffuser model"""
    return LightweightBrainDiffuser(input_dim=input_dim, device=device, **kwargs)


# Export main classes
__all__ = [
    'LightweightBrainDiffuser',
    'LightweightVDVAE',
    'LightweightDiffusion',
    'create_lightweight_brain_diffuser'
]
